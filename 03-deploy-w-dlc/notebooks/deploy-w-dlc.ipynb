{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b21238",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum update -y\n",
    "!sudo yum install amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y\n",
    "!sudo yum update -y\n",
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5addbc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT SAKEMAKER\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "# init session to connect to other aws ressource\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# create session bucket to store project artefacts\n",
    "sagemaker_session_bucket = \"sagemaker-bert2bert\"\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# IAM role to allow connection to session bucket and create model deployment\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNALOAD ORGINAL MODEL\n",
    "\n",
    "repository = \"mrm8488/bert2bert_shared-german-finetuned-summarization\"\n",
    "model_id=repository.split(\"/\")[-1]\n",
    "s3_location=f\"s3://{sess.default_bucket()}/{model_id}/model.tar.gz\"\n",
    "\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/$repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92a8894",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/inference.py\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'code/inference.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwritefile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcode/inference.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# INIT INFERENCE SCRIPT FOR TEST\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport torch\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Activate eval mode for inference\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.eval()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# Load quantized model and tokenizer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef model_fn(model_dir):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model_8bit = AutoModelForSeq2SeqLM.from_pretrained(model_dir, load_in_8bit=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    tokenizer = AutoTokenizer.from_pretrained(model_dir)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return model_8bit, tokenizer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mdef predict_fn(data, model_and_tokenizer):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    model, tokenizer = model_and_tokenizer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    text = data.pop(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, data)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    encoded_input = tokenizer(text, return_tensors=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    output_sequences = model.generate(input_ids=encoded_input[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m].cuda(), **data)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2493\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2492\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2493\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2496\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/magics/osm.py:854\u001b[0m, in \u001b[0;36mOSMagics.writefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m    853\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mappend \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    855\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(cell)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'code/inference.py'"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "# INIT INFERENCE SCRIPT FOR TEST\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Activate eval mode for inference\n",
    "model.eval()\n",
    "\n",
    "# Load quantized model and tokenizer\n",
    "def model_fn(model_dir):\n",
    "    model_8bit = AutoModelForSeq2SeqLM.from_pretrained(model_dir, load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    return model_8bit, tokenizer\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    text = data.pop(\"inputs\", data)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output_sequences = model.generate(input_ids=encoded_input['input_ids'].cuda(), **data)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "\n",
    "bitsandbytes\n",
    "accelerate\n",
    "git+https://github.com/huggingface/transformers.git@main#egg=transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY MODEL to PROJECT FOR DEPLOYMENT\n",
    "!cp -r code/ $model_id/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $model_id\n",
    "!tar zcvf model.tar.gz *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58714951",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp model.tar.gz $s3_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23113aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLOY MODEL\n",
    "\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39'\n",
    "\n",
    "# Define the Hugging Face model\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.26.0',  .\n",
    "    pytorch_version='1.13.1',  \n",
    "    py_version='py39',  \n",
    "    entry_point='inference.py', \n",
    "    role=role,  s\n",
    "    model_data=s3_location, # load from session bucket\n",
    "    image_uri=image_uri,\n",
    ")\n",
    "\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "\n",
    "endpoint_name = 'b2b-summarization-test'\n",
    "\n",
    "# helper to track deployment time\n",
    "start_time = time.time()\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "# helper to track deployment time\n",
    "deployment_duration = time.time() - start_time\n",
    "print(f\"Deployment completed in {deployment_duration} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab20a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST MODEL DEPLOYMENT\n",
    "\n",
    "data = {\n",
    "    \"inputs\": \"Veränderungen und Innovationen sind allgegenwärtig, eine kontinuierliche und nachhaltige Weiterentwicklung der Mitarbeitenden ist von zentraler Bedeutung. Hier setzt das Praxis-Coaching an, denn es ist eine wirkungsvolle Methode, um individuelle Lern- und Entwicklungsprozesse zu fördern und zu gestalten. Das Praxis-Coaching Data Science unterstützt Sie dabei, den Praxistransfer in den Unternehmenskontext zu optimieren und die Rolle des Data Scientists in Ihrer Arbeitsumgebung zu etablieren. Es hilft Ihnen dabei, an die Themen des Seminars praxisnah anzuknüpfen, und ermöglicht im virtuellen 1:1-Raum mit dem:der Coach:in den Austausch für individuelle Impulse und Tipps. Der Ansatz dieses Coachings beinhaltet weder eine Wissensvermittlung noch eine Unternehmensberatung. Es geht hier in erster Linie darum, Ihre Rolle als Data Scientist gemeinsam mit dem:der Coach:in zu analysieren und Ihr Selbstvertrauen in diesem Bereich zu fördern. Das Praxis-Coaching Data Science adressiert genau dieses Dilemma: Es bringt Ihre Daten-Ideen und Ihr Wissen aus den Fachbereichen mit der Erfahrung unserer Expert:innen bei der erfolgreichen Implementierung von Datenprojekten zusammen. Egal ob eine oder zehn Stunden - buchen Sie über unser Anfrageformular ein flexibel nutzbares Kontingent mit unseren Trainer:innen. Preis pro Stunde: 250€ (297,50€ inkl MwSt).\"\n",
    "    \"Geben Sie im nächsten Schritt bei „Anfragen“ im Bemerkungsfeld an, bei welchem:welcher Trainer:in Sie gerne das Coaching durchführen wollen und wie viele Stunden Sie buchen möchten. Der:Die Trainer:in meldet sich daraufhin bei Ihnen.\"\n",
    "    \"Inhalte\"\n",
    "    \"Das Praxis-Coaching Data Science unterstützt Sie bei den folgenden Möglichkeiten:\"\n",
    "    \"Entwicklung: Etablierung Ihrer Jobrolle als Data Scientist.\"\n",
    "    \"Follow-up: Anknüpfung an die Kursinhalte und Reflexion für den maximalen Praxistransfer.\"\n",
    "    \"Entfaltung: Individuelle Impulse und Tipps für die Umsetzung von Maßnahmen in Ihrem Businesskontext.\"\n",
    "    \"Ihr Nutzen\"\n",
    "    \"Nutzen Sie dieses Praxis-Coaching für die Förderung und Gestaltung Ihres individuellen Lern- und Entwicklungsprozesses:\"\n",
    "    \"Schaffen Sie im 1:1 mit dem Experten Klarheit für Ihre neue Rolle als Data Scientist und stärken Sie dieses Mindset für sich und Ihr Team.\",\n",
    "}\n",
    "\n",
    "response = predictor.predict(data=data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN UP\n",
    "predictor.delete_endpoint()\n",
    "sagemaker_session.delete_model(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
