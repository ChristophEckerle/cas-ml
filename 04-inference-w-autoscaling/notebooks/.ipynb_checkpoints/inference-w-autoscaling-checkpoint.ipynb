{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum update -y\n",
    "!sudo yum install amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y\n",
    "!sudo yum update -y\n",
    "!sudo yum install git-lfs -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT SAKEMAKER\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "# init session to connect to other aws ressource\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# create session bucket to store project artefacts\n",
    "sagemaker_session_bucket = \"sagemaker-bert2bert\"\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# IAM role to allow connection to session bucket and create model deployment\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNALOAD ORGINAL MODEL\n",
    "\n",
    "repository = \"mrm8488/bert2bert_shared-german-finetuned-summarization\"\n",
    "model_id=repository.split(\"/\")[-1]\n",
    "s3_location=f\"s3://{sess.default_bucket()}/{model_id}/model.tar.gz\"\n",
    "\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/$repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badfe76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "\n",
    "# INIT INFERENCE SCRIPT FOR TEST\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Activate eval mode for inference\n",
    "model.eval()\n",
    "\n",
    "# Load quantized model and tokenizer\n",
    "def model_fn(model_dir):\n",
    "    model_8bit = AutoModelForSeq2SeqLM.from_pretrained(model_dir, load_in_8bit=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    return model_8bit, tokenizer\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "    text = data.pop(\"inputs\", data)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output_sequences = model.generate(input_ids=encoded_input['input_ids'].cuda(), **data)\n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a12253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPLOY MODEL\n",
    "\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39'\n",
    "\n",
    "# Define the Hugging Face model\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.26.0',  .\n",
    "    pytorch_version='1.13.1',  \n",
    "    py_version='py39',  \n",
    "    entry_point='inference.py', \n",
    "    role=role,  s\n",
    "    model_data=s3_location, # load from session bucket\n",
    "    image_uri=image_uri,\n",
    ")\n",
    "\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "\n",
    "endpoint_name = 'b2b-summarization-test'\n",
    "\n",
    "# helper to track deployment time\n",
    "start_time = time.time()\n",
    "\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "# helper to track deployment time\n",
    "deployment_duration = time.time() - start_time\n",
    "print(f\"Deployment completed in {deployment_duration} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d2500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST MODEL DEPLOYMENT\n",
    "\n",
    "data = {\n",
    "    \"inputs\": \"Veränderungen und Innovationen sind allgegenwärtig, eine kontinuierliche und nachhaltige Weiterentwicklung der Mitarbeitenden ist von zentraler Bedeutung. Hier setzt das Praxis-Coaching an, denn es ist eine wirkungsvolle Methode, um individuelle Lern- und Entwicklungsprozesse zu fördern und zu gestalten. Das Praxis-Coaching Data Science unterstützt Sie dabei, den Praxistransfer in den Unternehmenskontext zu optimieren und die Rolle des Data Scientists in Ihrer Arbeitsumgebung zu etablieren. Es hilft Ihnen dabei, an die Themen des Seminars praxisnah anzuknüpfen, und ermöglicht im virtuellen 1:1-Raum mit dem:der Coach:in den Austausch für individuelle Impulse und Tipps. Der Ansatz dieses Coachings beinhaltet weder eine Wissensvermittlung noch eine Unternehmensberatung. Es geht hier in erster Linie darum, Ihre Rolle als Data Scientist gemeinsam mit dem:der Coach:in zu analysieren und Ihr Selbstvertrauen in diesem Bereich zu fördern. Das Praxis-Coaching Data Science adressiert genau dieses Dilemma: Es bringt Ihre Daten-Ideen und Ihr Wissen aus den Fachbereichen mit der Erfahrung unserer Expert:innen bei der erfolgreichen Implementierung von Datenprojekten zusammen. Egal ob eine oder zehn Stunden - buchen Sie über unser Anfrageformular ein flexibel nutzbares Kontingent mit unseren Trainer:innen. Preis pro Stunde: 250€ (297,50€ inkl MwSt).\"\n",
    "    \"Geben Sie im nächsten Schritt bei „Anfragen“ im Bemerkungsfeld an, bei welchem:welcher Trainer:in Sie gerne das Coaching durchführen wollen und wie viele Stunden Sie buchen möchten. Der:Die Trainer:in meldet sich daraufhin bei Ihnen.\"\n",
    "    \"Inhalte\"\n",
    "    \"Das Praxis-Coaching Data Science unterstützt Sie bei den folgenden Möglichkeiten:\"\n",
    "    \"Entwicklung: Etablierung Ihrer Jobrolle als Data Scientist.\"\n",
    "    \"Follow-up: Anknüpfung an die Kursinhalte und Reflexion für den maximalen Praxistransfer.\"\n",
    "    \"Entfaltung: Individuelle Impulse und Tipps für die Umsetzung von Maßnahmen in Ihrem Businesskontext.\"\n",
    "    \"Ihr Nutzen\"\n",
    "    \"Nutzen Sie dieses Praxis-Coaching für die Förderung und Gestaltung Ihres individuellen Lern- und Entwicklungsprozesses:\"\n",
    "    \"Schaffen Sie im 1:1 mit dem Experten Klarheit für Ihre neue Rolle als Data Scientist und stärken Sie dieses Mindset für sich und Ihr Team.\",\n",
    "}\n",
    "\n",
    "response = predictor.predict(data=data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG AUTO-SCALING-TARGET \n",
    "\n",
    "import boto3\n",
    "\n",
    "asg_client = boto3.client('application-autoscaling')\n",
    "\n",
    "resource_id=f\"endpoint/{predictor.endpoint_name}/variant/AllTraffic\"\n",
    "\n",
    "# scaling config\n",
    "response = asg_client.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', \n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    MinCapacity=1, #TODO: scale to zero\n",
    "    MaxCapacity=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG AUTO-SCALING-POLICY \n",
    "\n",
    "response = asg_client.put_scaling_policy(\n",
    "    PolicyName=f'Request-ScalingPolicy-{predictor.endpoint_name}',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 5.0, # Threshold\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',\n",
    "        },\n",
    "        'ScaleInCooldown': 300, # time in seconds to scale in\n",
    "        'ScaleOutCooldown': 60 # time in seconds to scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b39a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST AUTO-SCALING\n",
    "\n",
    "import time\n",
    "\n",
    "request_duration_in_seconds = 4*65\n",
    "end_time = time.time() + request_duration_in_seconds\n",
    "\n",
    "print(f\"test will run {request_duration_in_seconds} seconds\")\n",
    "\n",
    "while time.time() < end_time:\n",
    "    predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ba212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT CLOUDWATCH GRAPH \n",
    "print(f\"https://console.aws.amazon.com/cloudwatch/home?region={aws_region}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'InvocationsPerInstance~'EndpointName~'{predictor.endpoint_name}~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'{aws_region}~start~'-PT15M~end~'P0D~stat~'SampleCount~period~60);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae6b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT NUMBER OF INSTANCES\n",
    "bt_sm = boto3.client('sagemaker')\n",
    "response = bt_sm.describe_endpoint(EndpointName=predictor.endpoint_name)\n",
    "print(f\"Endpoint {response['EndpointName']} has \\nCurrent Instance Count: {response['ProductionVariants'][0]['CurrentInstanceCount']}\\nWith a desired instance count of {response['ProductionVariants'][0]['DesiredInstanceCount']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN UP\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
